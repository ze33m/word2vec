{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce103786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import string\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba22210",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "\n",
    "epochs = 50\n",
    "lr = 1e-2\n",
    "batch_size = 16\n",
    "embed_size = 300\n",
    "path = 'trans.txt'\n",
    "sw = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ffe737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_text(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().replace('\\n', ' ')\n",
    "\n",
    "        text = text.lower()\n",
    "        \n",
    "        text = text.replace('\\xa0', ' ')\n",
    "\n",
    "        text = re.sub(r'спикер (\\d+|\\?):', '', text)\n",
    "\n",
    "        for punct in string.punctuation +'«»':\n",
    "            text = text.replace(punct, ' ')\n",
    "\n",
    "        while '  ' in text:\n",
    "            text = text.replace('  ', ' ')\n",
    "\n",
    "        raw_tokens = word_tokenize(text)\n",
    "        \n",
    "        tokens = []\n",
    "\n",
    "        for token in raw_tokens:\n",
    "            if token not in sw and token.isnumeric() == False:\n",
    "                tokens.append(token)\n",
    "\n",
    "        return tokens\n",
    "        \n",
    "tokens = preprocess_raw_text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd79fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, tokens, window_size=window_size):\n",
    "        self.pairs = []\n",
    "        for i in range(window_size, len(tokens) - window_size):\n",
    "            target = tokens[i]\n",
    "            \n",
    "            context = tokens[i-window_size:i + window_size + 1]\n",
    "\n",
    "            context.remove(target)\n",
    "\n",
    "            for word in context:\n",
    "                self.pairs.append((target, word))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)   \n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.pairs[i]\n",
    "\n",
    "    \n",
    "dataset = SkipGramDataset(tokens=tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c64c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491, 'крош')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(tokens))\n",
    "wtoi = {i : word for word, i in enumerate(vocab)}\n",
    "itow = {word : i for word, i in enumerate(vocab)}\n",
    "wtoi['крош'],itow[491]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ded0e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "for target, context in dataset:\n",
    "    X.append(wtoi[target])\n",
    "    y.append(wtoi[context])\n",
    "\n",
    "X_train = torch.LongTensor(X)\n",
    "y_train = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba9d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size ):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.linear = nn.Linear(embed_size, vocab_size)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(self.embed(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGramModel(len(vocab), embed_size=embed_size)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd4c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maxim\\porjs\\word2vec\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch num: 1, loss value: 6678.929\n",
      "Epoch num: 2, loss value: 6629.781\n",
      "Epoch num: 3, loss value: 6603.886\n",
      "Epoch num: 4, loss value: 6599.707\n",
      "Epoch num: 5, loss value: 6598.539\n",
      "Epoch num: 6, loss value: 6598.410\n",
      "Epoch num: 7, loss value: 6598.361\n",
      "Epoch num: 8, loss value: 6598.067\n",
      "Epoch num: 9, loss value: 6598.060\n",
      "Epoch num: 10, loss value: 6597.998\n",
      "Epoch num: 11, loss value: 6597.998\n",
      "Epoch num: 12, loss value: 6597.992\n",
      "Epoch num: 13, loss value: 6597.935\n",
      "Epoch num: 14, loss value: 6597.811\n",
      "Epoch num: 15, loss value: 6597.811\n",
      "Epoch num: 16, loss value: 6597.936\n",
      "Epoch num: 17, loss value: 6597.873\n",
      "Epoch num: 18, loss value: 6597.935\n",
      "Epoch num: 19, loss value: 6597.874\n",
      "Epoch num: 20, loss value: 6598.066\n",
      "Epoch num: 21, loss value: 6598.079\n",
      "Epoch num: 22, loss value: 6598.059\n",
      "Epoch num: 23, loss value: 6598.080\n",
      "Epoch num: 24, loss value: 6598.091\n",
      "Epoch num: 25, loss value: 6598.061\n",
      "Epoch num: 26, loss value: 6597.873\n",
      "Epoch num: 27, loss value: 6597.811\n",
      "Epoch num: 28, loss value: 6597.826\n",
      "Epoch num: 29, loss value: 6597.811\n",
      "Epoch num: 30, loss value: 6597.811\n",
      "Epoch num: 31, loss value: 6597.811\n",
      "Epoch num: 32, loss value: 6597.811\n",
      "Epoch num: 33, loss value: 6597.848\n",
      "Epoch num: 34, loss value: 6597.811\n",
      "Epoch num: 35, loss value: 6597.811\n",
      "Epoch num: 36, loss value: 6597.873\n",
      "Epoch num: 37, loss value: 6597.935\n",
      "Epoch num: 38, loss value: 6597.874\n",
      "Epoch num: 39, loss value: 6597.873\n",
      "Epoch num: 40, loss value: 6597.873\n",
      "Epoch num: 41, loss value: 6597.873\n",
      "Epoch num: 42, loss value: 6597.873\n",
      "Epoch num: 43, loss value: 6597.873\n",
      "Epoch num: 44, loss value: 6597.873\n",
      "Epoch num: 45, loss value: 6597.874\n",
      "Epoch num: 46, loss value: 6597.873\n",
      "Epoch num: 47, loss value: 6597.873\n",
      "Epoch num: 48, loss value: 6597.873\n",
      "Epoch num: 49, loss value: 6597.873\n",
      "Epoch num: 50, loss value: 6597.936\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "\n",
    "        x = X_train[i:i+batch_size]\n",
    "        y = y_train[i:i+batch_size]\n",
    "\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_func(y_pred, y.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch num: {epoch+1}, loss value: {total_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74044bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
