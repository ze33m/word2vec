# word2vec with negative sampling
Подробно про word2vec можно узнать [тут](https://www.youtube.com/watch?v=HL0X4QJ23Cw&t=4777s). Если вкратце, 
мы пытаемся создать модель, которая будет преобразовывать слова в многомерные векторы (я выбрал размер векторов 128).
Однако векторы должны нести семантический смысл. То есть векторы слов, близких по значению, должны быть очень близки (по косинусной схожести).
Для этого максимизируем:

![opt.png](src/opt.png)

Обведенное зеленым означает, что мы хотим сблизить похожие по значению векторы слов, а обведенное желтым - отдалить непохожие 

- как понять, какие слова близкие по значению?

Мы делаем, на самом деле, наивное предположение о том, что слова, находящиеся рядом, похожи. Для этого идем по каждому 
токену (слову) в датасете, и все токены, находящиеся рядом с ним - близкие.
А вот далекие по значению слова берутся из распределения всех встречающихся слов.
То есть просто рандомное слово из словаря, причем чем чаще оно встречается, тем более вероятно оно будет в роли негативного примера.
Сделано это для того, чтобы отдалить самые часто встречающиеся слова по типу 'это', 'на' от слов, которые реально несут за собой какой-то смысл.

# Пайплайн
## Предобработка исходного датасета
За датасет была взята [0x7o/taiga](https://huggingface.co/datasets/0x7o/taiga). Первичная предобработка реализована в [prep.py](prep.py). Все слова приведены к нижнему регистру, убрана пунктуация, цифры, лишние пробелы. После этого каждая строка была преобразована в список слов. 
![prep](src/prep.png)
*здесь и в последующих картинках '/' на конце означает локальную директорию*

## Преобразование токенов в числа
Так как в конечном итоге взаимодействовать с токенами мы будем с помощью чисел через эмбеддинги, а преобразовывание str -> int при каждом обращении к токену замедлит процесс обучения, я преобразовал все токены в int заранее. Словарь, в котором написано, какой токен какому числу соответствует, сохранен в виде json файла. Размер словаря получился в 2495767 слов. Весь процесс описан в [convert_to_int.py](convert_to_int.py) 
![convert](src/convert_to_int.png)

## Извлечение пар [target, context]
Датасет, которым мы будем пользоваться во время обучения, состоит из двух колонок. target - целевое слово, и context - слово, находящееся с target в одном контексте (в пределах window_size). 

В [pairs_dataset.py](pairs_dataset.py) эти пары вычисляются и поточно записываются по 50млн пар в каждый шард. Исходя из размера словаря можем хранить не int64, а в int32.

![get pairs](src/get_pairs.png)

Про негативные примеры (negatives) - их мы будем генерировать с помощью заранее подготовленной unigram table. Это выгодней, чем считать их на лету, каждый раз считая распределение заново.

## Итерационный датасет 
В [dataset.py](dataset.py) реализован поточный датасет, проходящий по одному parquet файлу из 50млн пар. Так же в нем мы используем полученное в [unigram_table.py](unigram_table.py) распределение слов для семплера негативных примеров. 

## Про модель
Сама модель реализована в [word2vec.py](word2vec.py). В нем реализована сама формула оптимизации и есть метод для поиска K ближайших соседей. Ембеддинги я использовал разреженные(sparse) для экономии памяти. 

## Обучение модели
Реализовано в [train.py](train.py). Проходимся по каждому шарду. 

батч в 300К * 167 итераций = 1 паркет шард = 50млн пар таргет+контекст

Характеристики кластера для обучения:
* GPU 1 × Tesla T4  16 ГБ
* RAM 32 ГБ
* vCPU 4 ядра

Одна эпоха обучения с такой конфигурацией занимает примерно 13 часов. Но я решил остановиться на 200ом шарде, когда лосс упал до 0.450. Поэтому у меня обучение заняло примерно 4 часа.
![get pairs](src/loss.png)

Получились очень неплохие ембеддинги. Примеры соседей можно посмотреть [тут](examples.txt)